{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoTVCUtTQL6c"
   },
   "source": [
    "# TP 1: LDA/QDA y optimización matemática de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kL_4etdeizy"
   },
   "source": [
    "# Intro teórica\n",
    "\n",
    "## Definición: Clasificador Bayesiano\n",
    "\n",
    "Sean $k$ poblaciones, $x \\in \\mathbb{R}^p$ puede pertenecer a cualquiera $g \\in \\mathcal{G}$ de ellas. Bajo un esquema bayesiano, se define entonces $\\pi_j \\doteq P(G = j)$ la probabilidad *a priori* de que $X$ pertenezca a la clase *j*, y se **asume conocida** la distribución condicional de cada observable dado su clase $f_j \\doteq f_{X|G=j}$.\n",
    "\n",
    "De esta manera dicha probabilidad *a posteriori* resulta\n",
    "$$\n",
    "P(G|_{X=x} = j) = \\frac{f_{X|G=j}(x) \\cdot p_G(j)}{f_X(x)} \\propto f_j(x) \\cdot \\pi_j\n",
    "$$\n",
    "\n",
    "La regla de decisión de Bayes es entonces\n",
    "$$\n",
    "H(x) \\doteq \\arg \\max_{g \\in \\mathcal{G}} \\{ P(G|_{X=x} = j) \\} = \\arg \\max_{g \\in \\mathcal{G}} \\{ f_j(x) \\cdot \\pi_j \\}\n",
    "$$\n",
    "\n",
    "es decir, se predice a $x$ como perteneciente a la población $j$ cuya probabilidad a posteriori es máxima.\n",
    "\n",
    "*Ojo, a no desesperar! $\\pi_j$ no es otra cosa que una constante prefijada, y $f_j$ es, en su esencia, un campo escalar de $x$ a simplemente evaluar.*\n",
    "\n",
    "## Distribución condicional\n",
    "\n",
    "Para los clasificadores de discriminante cuadrático y lineal (QDA/LDA) se asume que $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma_j)$, es decir, se asume que cada población sigue una distribución normal.\n",
    "\n",
    "Por definición, se tiene entonces que para una clase $j$:\n",
    "$$\n",
    "f_j(x) = \\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma_j|^\\frac{1}{2}} e^{- \\frac{1}{2}(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)}\n",
    "$$\n",
    "\n",
    "Aplicando logaritmo (que al ser una función estrictamente creciente no afecta el cálculo de máximos/mínimos), queda algo mucho más práctico de trabajar:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "\n",
    "Observar que en este caso $C=-\\frac{p}{2} \\log(2\\pi)$, pero no se tiene en cuenta ya que al tener una constante aditiva en todas las clases, no afecta al cálculo del máximo.\n",
    "\n",
    "## LDA\n",
    "\n",
    "En el caso de LDA se hace una suposición extra, que es $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma)$, es decir que las poblaciones no sólo siguen una distribución normal sino que son de igual matriz de covarianzas. Reemplazando arriba se obtiene entonces:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "\n",
    "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva y, distribuyendo y reagrupando términos sobre $(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$ se obtiene finalmente:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
    "$$\n",
    "\n",
    "## Entrenamiento/Ajuste\n",
    "\n",
    "Obsérvese que para ambos modelos, ajustarlos a los datos implica estimar los parámetros $(\\mu_j, \\Sigma_j) \\; \\forall j = 1, \\dots, k$ en el caso de QDA, y $(\\mu_j, \\Sigma)$ para LDA.\n",
    "\n",
    "Estos parámetros se estiman por máxima verosimilitud, de manera que los estimadores resultan:\n",
    "\n",
    "* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n",
    "* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n",
    "* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n",
    "* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n",
    "\n",
    "Es importante notar que si bien todos los $\\mu, \\Sigma$ deben ser estimados, la distribución *a priori* puede no inferirse de los datos sino asumirse previamente, utilizándose como entrada del modelo.\n",
    "\n",
    "## Predicción\n",
    "\n",
    "Para estos modelos, al igual que para cualquier clasificador Bayesiano del tipo antes visto, la estimación de la clase es por método *plug-in* sobre la regla de decisión $H(x)$, es decir devolver la clase que maximiza $\\hat{f}_j(x) \\cdot \\hat{\\pi}_j$, o lo que es lo mismo $\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV8OF-SlPHbD"
   },
   "source": [
    "# Código provisto\n",
    "\n",
    "Con el fin de no retrasar al alumno con cuestiones estructurales y/o secundarias al tema que se pretende tratar, se provee una base de código que **no es obligatoria de usar** pero se asume que resulta resulta beneficiosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "PrDdJRypNB-y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.linalg as LA\n",
    "from scipy.linalg import cholesky, solve_triangular\n",
    "from scipy.linalg.lapack import dtrtri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cPL33WIN2HA"
   },
   "source": [
    "## Base code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ewg5e0hsNTQC"
   },
   "outputs": [],
   "source": [
    "class BaseBayesianClassifier:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def _estimate_a_priori(self, y):\n",
    "    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
    "    #print(\"a_priori shape= \", a_priori.shape)\n",
    "    #print(\"a_priori= \", a_priori)\n",
    "    # Q3: para que sirve bincount?\n",
    "    # R3: Crea un vector con la cantidad de ocurrencias de cada entero dentro de y\n",
    "    return np.log(a_priori)\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate all needed parameters for given model\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def fit(self, X, y, a_priori=None):\n",
    "    # if it's needed, estimate a priori probabilities\n",
    "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
    "\n",
    "    # now that everything else is in place, estimate all needed parameters for given model\n",
    "    self._fit_params(X, y)\n",
    "    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n",
    "    # R4: porque el _fit_params depende de la priori, y la priori se calcula antes\n",
    "\n",
    "  def predict(self, X):\n",
    "    # this is actually an individual prediction encased in a for-loop\n",
    "    m_obs = X.shape[1]\n",
    "    y_hat = np.empty(m_obs, dtype=int)\n",
    "    #print(\"y_hat shape= \", y_hat.shape)\n",
    "    #print(\"y_hat= \", y_hat)\n",
    "\n",
    "    for i in range(m_obs):\n",
    "      y_hat[i] = self._predict_one(X[:,i].reshape(-1,1))\n",
    "\n",
    "    # return prediction as a row vector (matching y)\n",
    "    return y_hat.reshape(1,-1)\n",
    "\n",
    "  def _predict_one(self, x):\n",
    "    # calculate all log posteriori probabilities (actually, +C)\n",
    "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n",
    "                  in enumerate(self.log_a_priori) ]\n",
    "    # return the class that has maximum a posteriori probability\n",
    "    #print(\"log_posteriori= \", log_posteriori)\n",
    "    return np.argmax(log_posteriori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Rz2FC7A5NUpN"
   },
   "outputs": [],
   "source": [
    "class QDA(BaseBayesianClassifier):\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate each covariance matrix\n",
    "    print(\"X shape= \", X.shape)\n",
    "    print(\"y shape= \", y.shape)\n",
    "    # print(\"y.flatten\", y.flatten())\n",
    "    self.inv_covs = [LA.inv(np.cov(X[:,y.flatten()==idx], bias=True))\n",
    "                      for idx in range(len(self.log_a_priori))]\n",
    "    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n",
    "    # R5: porque y es un vector columna y no se puede comparar con un vector fila. \n",
    "    # Flatten en este caso lo convierte en arreglo de 1D y entonces si se puede hacer la comparacion con idx.\n",
    "    # Q6: por que se usa bias=True en vez del default bias=False?\n",
    "    # R6: porque bias=False hace que la matriz de covarianza sea insesgada.\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "    # Q7: que hace axis=1? por que no axis=0?\n",
    "    # R7: axis=1 indica que el promedio se debe calcular por columnas y no por filas.\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    inv_cov = self.inv_covs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "    return 0.5*np.log(LA.det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "9lZbID0WNV1Y"
   },
   "outputs": [],
   "source": [
    "class TensorizedQDA(QDA):\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        # ask plain QDA to fit params\n",
    "        super()._fit_params(X,y)\n",
    "\n",
    "        # stack onto new dimension\n",
    "        self.tensor_inv_cov = np.stack(self.inv_covs)\n",
    "        self.tensor_means = np.stack(self.means)\n",
    "\n",
    "    def _predict_log_conditionals(self,x):\n",
    "        unbiased_x = x - self.tensor_means\n",
    "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x\n",
    "\n",
    "        return 0.5*np.log(LA.det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()\n",
    "\n",
    "    def _predict_one(self, x):\n",
    "        # return the class that has maximum a posteriori probability\n",
    "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "i-WGGi_sQ-pT"
   },
   "outputs": [],
   "source": [
    "class QDA_Chol1(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.L_invs = [\n",
    "        LA.inv(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True))\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L_inv = self.L_invs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = L_inv @ unbiased_x\n",
    "\n",
    "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "i5DNLtYbQsHi"
   },
   "outputs": [],
   "source": [
    "class QDA_Chol2(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.Ls = [\n",
    "        cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True)\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L = self.Ls[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = solve_triangular(L, unbiased_x, lower=True)\n",
    "\n",
    "    return -np.log(L.diagonal().prod()) -0.5 * (y**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "v0dRvYVQRCgc"
   },
   "outputs": [],
   "source": [
    "class QDA_Chol3(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.L_invs = [\n",
    "        dtrtri(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True), lower=1)[0]\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L_inv = self.L_invs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = L_inv @ unbiased_x\n",
    "\n",
    "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCtrHQDuN6R4"
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "rasInBMFNzUH"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, fetch_openml, load_wine\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_iris_dataset():\n",
    "  data = load_iris()\n",
    "  X_full = data.data\n",
    "  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
    "  return X_full, y_full\n",
    "\n",
    "def get_penguins_dataset():\n",
    "    # get data\n",
    "    df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n",
    "\n",
    "    # drop non-numeric columns\n",
    "    df.drop(columns=[\"island\",\"sex\"], inplace=True)\n",
    "\n",
    "    # drop rows with missing values\n",
    "    mask = df.isna().sum(axis=1) == 0\n",
    "    df = df[mask]\n",
    "    tgt = tgt[mask]\n",
    "\n",
    "    return df.values, tgt.to_numpy().reshape(-1,1)\n",
    "\n",
    "def get_wine_dataset():\n",
    "    # get data\n",
    "    data = load_wine()\n",
    "    X_full = data.data\n",
    "    y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
    "    return X_full, y_full\n",
    "\n",
    "def get_letters_dataset():\n",
    "    # get data\n",
    "    letter = fetch_openml('letter', version=1, as_frame=False)\n",
    "    return letter.data, letter.target.reshape(-1,1)\n",
    "\n",
    "def label_encode(y_full):\n",
    "    return LabelEncoder().fit_transform(y_full.flatten()).reshape(y_full.shape)\n",
    "\n",
    "def split_transpose(X, y, test_size, random_state):\n",
    "    # X_train, X_test, y_train, y_test but all transposed\n",
    "    return [elem.T for elem in train_test_split(X, y, test_size=test_size, random_state=random_state)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybPkuBdDN42P"
   },
   "source": [
    "## Benchmarking\n",
    "\n",
    "Nota: esta clase fue creada bastante rápido y no pretende ser una plataforma súper confiable sobre la que basarse, sino más bien una herramienta simple con la que poder medir varios runs y agregar la información.\n",
    "\n",
    "En forma rápida, `warmup` es la cantidad de runs para warmup, `mem_runs` es la cantidad de runs en las que se mide el pico de uso de RAM y `n_runs` es la cantidad de runs en las que se miden tiempos.\n",
    "\n",
    "La razón por la que se separan es que medir memoria hace ~2.5x más lento cada run, pero al mismo tiempo se estabiliza mucho más rápido.\n",
    "\n",
    "**Importante:** tener en cuenta que los modelos que predicen en batch (usan `predict` directamente) deberían consumir, como mínimo, $n$ veces la memoria de los que predicen por observación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "nO4Py3CeNpKu"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from numpy.random import RandomState\n",
    "import tracemalloc\n",
    "\n",
    "RNG_SEED = 6553\n",
    "\n",
    "class Benchmark:\n",
    "    def __init__(self, X, y, n_runs=1000, warmup=100, mem_runs=100, test_sz=0.3, rng_seed=RNG_SEED, same_splits=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n = n_runs\n",
    "        self.warmup = warmup\n",
    "        self.mem_runs = mem_runs\n",
    "        self.test_sz = test_sz\n",
    "        self.det = same_splits\n",
    "        if self.det:\n",
    "            self.rng_seed = rng_seed\n",
    "        else:\n",
    "            self.rng = RandomState(rng_seed)\n",
    "\n",
    "        self.data = dict()\n",
    "\n",
    "        print(\"Benching params:\")\n",
    "        print(\"Total runs:\",self.warmup+self.mem_runs+self.n)\n",
    "        print(\"Warmup runs:\",self.warmup)\n",
    "        print(\"Peak Memory usage runs:\", self.mem_runs)\n",
    "        print(\"Running time runs:\", self.n)\n",
    "        approx_test_sz = int(self.y.size * self.test_sz)\n",
    "        print(\"Train size rows (approx):\",self.y.size - approx_test_sz)\n",
    "        print(\"Test size rows (approx):\",approx_test_sz)\n",
    "        print(\"Test size fraction:\",self.test_sz)\n",
    "\n",
    "    def bench(self, model_class, **kwargs):\n",
    "        name = model_class.__name__\n",
    "        time_data = np.empty((self.n, 3), dtype=float)  # train_time, test_time, accuracy\n",
    "        mem_data = np.empty((self.mem_runs, 2), dtype=float)  # train_peak_mem, test_peak_mem\n",
    "        rng = RandomState(self.rng_seed) if self.det else self.rng\n",
    "\n",
    "\n",
    "        for i in range(self.warmup):\n",
    "            # Instantiate model with error check for unsupported parameters\n",
    "            model = model_class(**kwargs)\n",
    "\n",
    "            # Generate current train-test split\n",
    "            X_train, X_test, y_train, y_test = split_transpose(\n",
    "                self.X, self.y,\n",
    "                test_size=self.test_sz,\n",
    "                random_state=rng\n",
    "            )\n",
    "            # Run training and prediction (timing or memory measurement not recorded)\n",
    "            model.fit(X_train, y_train)\n",
    "            model.predict(X_test)\n",
    "\n",
    "        for i in tqdm(range(self.mem_runs), total=self.mem_runs, desc=f\"{name} (MEM)\"):\n",
    "\n",
    "            model = model_class(**kwargs)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = split_transpose(\n",
    "                self.X, self.y,\n",
    "                test_size=self.test_sz,\n",
    "                random_state=rng\n",
    "            )\n",
    "\n",
    "            tracemalloc.start()\n",
    "\n",
    "            t1 = time.perf_counter()\n",
    "            model.fit(X_train, y_train)\n",
    "            t2 = time.perf_counter()\n",
    "\n",
    "            _, train_peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.reset_peak()\n",
    "\n",
    "            model.predict(X_test)\n",
    "            t3 = time.perf_counter()\n",
    "            _, test_peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "\n",
    "            mem_data[i,] = (\n",
    "                train_peak / (1024 * 1024),\n",
    "                test_peak / (1024 * 1024)\n",
    "            )\n",
    "\n",
    "        for i in tqdm(range(self.n), total=self.n, desc=f\"{name} (TIME)\"):\n",
    "            model = model_class(**kwargs)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = split_transpose(\n",
    "                self.X, self.y,\n",
    "                test_size=self.test_sz,\n",
    "                random_state=rng\n",
    "            )\n",
    "\n",
    "            t1 = time.perf_counter()\n",
    "            model.fit(X_train, y_train)\n",
    "            t2 = time.perf_counter()\n",
    "            preds = model.predict(X_test)\n",
    "            t3 = time.perf_counter()\n",
    "\n",
    "            time_data[i,] = (\n",
    "                (t2 - t1) * 1000,\n",
    "                (t3 - t2) * 1000,\n",
    "                (y_test.flatten() == preds.flatten()).mean()\n",
    "            )\n",
    "\n",
    "        self.data[name] = (time_data, mem_data)\n",
    "\n",
    "    def summary(self, baseline=None):\n",
    "        aux = []\n",
    "        for name, (time_data, mem_data) in self.data.items():\n",
    "            result = {\n",
    "                'model': name,\n",
    "                'train_mean_ms': time_data[:, 0].mean(),\n",
    "                'train_std_ms': time_data[:, 0].std(),\n",
    "                'test_mean_ms': time_data[:, 1].mean(),\n",
    "                'test_std_ms': time_data[:, 1].std(),\n",
    "                'mean_accuracy': time_data[:, 2].mean(),\n",
    "                'train_mem_mean_mb': mem_data[:, 0].mean(),\n",
    "                'train_mem_std_mb': mem_data[:, 0].std(),\n",
    "                'test_mem_mean_mb': mem_data[:, 1].mean(),\n",
    "                'test_mem_std_mb': mem_data[:, 1].std()\n",
    "            }\n",
    "            aux.append(result)\n",
    "        df = pd.DataFrame(aux).set_index('model')\n",
    "\n",
    "        if baseline is not None and baseline in self.data:\n",
    "            df['train_speedup'] = df.loc[baseline, 'train_mean_ms'] / df['train_mean_ms']\n",
    "            df['test_speedup'] = df.loc[baseline, 'test_mean_ms'] / df['test_mean_ms']\n",
    "            df['train_mem_reduction'] = df.loc[baseline, 'train_mem_mean_mb'] / df['train_mem_mean_mb']\n",
    "            df['test_mem_reduction'] = df.loc[baseline, 'test_mem_mean_mb'] / df['test_mem_mean_mb']\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb5VEpEugFXW"
   },
   "source": [
    "## Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLyr4-hdgJ7e",
    "outputId": "bfa9623f-2baf-4735-96b5-c714b244b8da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((178, 13), (178, 1))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# levantamos el dataset Wine, que tiene 13 features y 178 observaciones en total\n",
    "X_full, y_full = get_wine_dataset()\n",
    "\n",
    "X_full.shape, y_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZxQlFUSbgYHQ",
    "outputId": "dd396038-8bab-4ebd-b981-752526d8c98c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_full shape=  (178, 1)\n",
      "y_full_encoded shape=  (178, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([['class_0'],\n",
       "        ['class_0'],\n",
       "        ['class_0'],\n",
       "        ['class_0'],\n",
       "        ['class_0']], dtype='<U7'),\n",
       " array([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]], dtype=int64))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encodeamos a número las clases\n",
    "y_full_encoded = label_encode(y_full)\n",
    "print(\"y_full shape= \", y_full.shape)\n",
    "print(\"y_full_encoded shape= \", y_full_encoded.shape)\n",
    "\n",
    "y_full[:5], y_full_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pSBNNUOmgtsI",
    "outputId": "d29b11de-b5a9-4fa3-f009-d8780104fa64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benching params:\n",
      "Total runs: 6\n",
      "Warmup runs: 2\n",
      "Peak Memory usage runs: 2\n",
      "Running time runs: 2\n",
      "Train size rows (approx): 125\n",
      "Test size rows (approx): 53\n",
      "Test size fraction: 0.3\n"
     ]
    }
   ],
   "source": [
    "# generamos el benchmark\n",
    "# observar que son valores muy bajos de runs para que corra rápido ahora\n",
    "b = Benchmark(\n",
    "    X_full, y_full_encoded,\n",
    "    n_runs = 2,\n",
    "    warmup = 2,\n",
    "    mem_runs = 2,\n",
    "    test_sz = 0.3,\n",
    "    same_splits = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "2add89be4e944f4fbd91c5f1d459b5cb",
      "e0a05e28520841bd95e2aec19da024bd",
      "c406ae72dbf14856971c0ac3ad078062",
      "ca156cc465e1415b8df2a570abfa3ffe",
      "40416335cca64bbbad37afc46049363d",
      "4531ea3385d34454865e3c6ced188122",
      "a9bc2a66f8a84516a44d8d3ad8e885e2",
      "43ef5277fff74410bb85f09e64c37cfe",
      "d7276daf0b654112aab411ae3f14649f",
      "68a99e1ecb9741f688cb33bbfc19d5d2",
      "cec5bcb756694268abfc4eefad72f758",
      "0878ca0785b74f4fa8abce52c184ce33",
      "9cb5b6651bac47d0b99c9388def7d2e6",
      "ec4d02ef09c5492ab097ef02b9d3e2fb",
      "ed4b7065efa24e5da4f69c047647c047",
      "08dc6e2b043f409f8f8df2b63f6bc152",
      "f7bc1a202cde4351a6f98ae19393cf94",
      "caea67fec94c45a097cae720b582c68d",
      "c136c5327b804c74b0582ed04c8825dc",
      "b2d747a3bfdd4f4797f7e4b01964aa7b",
      "a5957e8514974187838fc2d43d548433",
      "38dcdaf521fe4043b7bc5fac762867e1"
     ]
    },
    "id": "zUciOjazhUu5",
    "outputId": "91fc0889-7a87-418e-9950-88371c912be9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n",
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8923e78e62414b9725be641440999d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA (MEM):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n",
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2448b436ee834c8cb6ea810857ec4408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA (TIME):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n",
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n"
     ]
    }
   ],
   "source": [
    "# bencheamos un par\n",
    "to_bench = [QDA]\n",
    "\n",
    "for model in to_bench:\n",
    "    b.bench(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "e3704aaa731c464a8d6c63c39d0b167a",
      "ee002c5199874f8db461bb1631f198ac",
      "9e38484f037e4f86be6b768459404767",
      "d670b24d966f4de1872524684ea98840",
      "74859eadc1ef4090a07e851f5949f4d6",
      "1f6553e18f4f460ebbc4b9705cfb9c3d",
      "44cc54751afe4e618d81e92e2e64ca58",
      "f1cdc4724c6d441cbebca60f1bb11aea",
      "4965b54e63794e0b80c19c150b072eda",
      "4c09e94a9e494e6daa6f59c23b8bcb4f",
      "ab3b1739c3bf4f3ba399e3a60d043ec8",
      "6bbe1b2d6f3846168244aca7ecf761f0",
      "eb6e454a891d493b88fe82442a9288e2",
      "3fad52f5ac204004afac2df2b55cbc7b",
      "2d6622ab061342f280f7fbcbc920bc00",
      "e7cdc4320bff45f6acf901b8bd9bdf1a",
      "bf6f0511eeca467c870c6a261712ca7e",
      "26d56661a9e4490c96ef5dd831fa904b",
      "ba742120ade4495d9f75b066c43b6ffb",
      "e38ba66c86764d578b97aa127e0cd9ce",
      "f87908723b244cd99144a4e1b7be8807",
      "568fd1f1b5984675bf1716520a63b7d1"
     ]
    },
    "id": "wpPhSSCNhlvG",
    "outputId": "a5302dc3-d947-47db-d96d-cf7a98152b0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n",
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eaa5bbb6d3b4e1782f7e53070ae6075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TensorizedQDA (MEM):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n",
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff983d44ce84d03ac9f96ef26b9e50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TensorizedQDA (TIME):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n",
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n"
     ]
    }
   ],
   "source": [
    "# como es una clase, podemos seguir bencheando más después\n",
    "b.bench(TensorizedQDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "bZ5-vowshr5c",
    "outputId": "f17bc091-0cf5-42b9-cd9a-1e3d61e824b0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mean_ms</th>\n",
       "      <th>train_std_ms</th>\n",
       "      <th>test_mean_ms</th>\n",
       "      <th>test_std_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_mem_mean_mb</th>\n",
       "      <th>train_mem_std_mb</th>\n",
       "      <th>test_mem_mean_mb</th>\n",
       "      <th>test_mem_std_mb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>1.49945</td>\n",
       "      <td>0.15025</td>\n",
       "      <td>2.31385</td>\n",
       "      <td>0.03935</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.009177</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>2.08250</td>\n",
       "      <td>0.41690</td>\n",
       "      <td>1.24170</td>\n",
       "      <td>0.08850</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.019810</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.013051</td>\n",
       "      <td>0.000299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_mean_ms  train_std_ms  test_mean_ms  test_std_ms  \\\n",
       "model                                                                   \n",
       "QDA                  1.49945       0.15025       2.31385      0.03935   \n",
       "TensorizedQDA        2.08250       0.41690       1.24170      0.08850   \n",
       "\n",
       "               mean_accuracy  train_mem_mean_mb  train_mem_std_mb  \\\n",
       "model                                                               \n",
       "QDA                 0.981481           0.019324          0.000088   \n",
       "TensorizedQDA       0.953704           0.019810          0.000597   \n",
       "\n",
       "               test_mem_mean_mb  test_mem_std_mb  \n",
       "model                                             \n",
       "QDA                    0.009177         0.000900  \n",
       "TensorizedQDA          0.013051         0.000299  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hacemos un summary\n",
    "b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "09eKXqlXhwL-",
    "outputId": "d42734a6-6fd8-4b15-da84-144d4923113f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mean_ms</th>\n",
       "      <th>test_mean_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>1.49945</td>\n",
       "      <td>2.31385</td>\n",
       "      <td>0.981481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>2.08250</td>\n",
       "      <td>1.24170</td>\n",
       "      <td>0.953704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_mean_ms  test_mean_ms  mean_accuracy\n",
       "model                                                    \n",
       "QDA                  1.49945       2.31385       0.981481\n",
       "TensorizedQDA        2.08250       1.24170       0.953704"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# son muchos datos! nos quedamos con un par nomás\n",
    "summ = b.summary()\n",
    "\n",
    "# como es un pandas DataFrame, subseteamos columnas fácil\n",
    "summ[['train_mean_ms', 'test_mean_ms','mean_accuracy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "EopB9574h8I5",
    "outputId": "c2bd86ab-3ba3-456d-c99b-d3bd131af75f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mean_ms</th>\n",
       "      <th>train_std_ms</th>\n",
       "      <th>test_mean_ms</th>\n",
       "      <th>test_std_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_mem_mean_mb</th>\n",
       "      <th>train_mem_std_mb</th>\n",
       "      <th>test_mem_mean_mb</th>\n",
       "      <th>test_mem_std_mb</th>\n",
       "      <th>train_speedup</th>\n",
       "      <th>test_speedup</th>\n",
       "      <th>train_mem_reduction</th>\n",
       "      <th>test_mem_reduction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>1.49945</td>\n",
       "      <td>0.15025</td>\n",
       "      <td>2.31385</td>\n",
       "      <td>0.03935</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.009177</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>2.08250</td>\n",
       "      <td>0.41690</td>\n",
       "      <td>1.24170</td>\n",
       "      <td>0.08850</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.019810</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.013051</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.720024</td>\n",
       "      <td>1.863453</td>\n",
       "      <td>0.975472</td>\n",
       "      <td>0.703204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_mean_ms  train_std_ms  test_mean_ms  test_std_ms  \\\n",
       "model                                                                   \n",
       "QDA                  1.49945       0.15025       2.31385      0.03935   \n",
       "TensorizedQDA        2.08250       0.41690       1.24170      0.08850   \n",
       "\n",
       "               mean_accuracy  train_mem_mean_mb  train_mem_std_mb  \\\n",
       "model                                                               \n",
       "QDA                 0.981481           0.019324          0.000088   \n",
       "TensorizedQDA       0.953704           0.019810          0.000597   \n",
       "\n",
       "               test_mem_mean_mb  test_mem_std_mb  train_speedup  test_speedup  \\\n",
       "model                                                                           \n",
       "QDA                    0.009177         0.000900       1.000000      1.000000   \n",
       "TensorizedQDA          0.013051         0.000299       0.720024      1.863453   \n",
       "\n",
       "               train_mem_reduction  test_mem_reduction  \n",
       "model                                                   \n",
       "QDA                       1.000000            1.000000  \n",
       "TensorizedQDA             0.975472            0.703204  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# podemos setear un baseline para que fabrique columnas de comparación\n",
    "summ = b.summary(baseline='QDA')\n",
    "\n",
    "summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "z0qeE1gviFLZ",
    "outputId": "26f288da-88c0-4568-d4cb-f3d60bf5045c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mean_ms</th>\n",
       "      <th>test_mean_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_speedup</th>\n",
       "      <th>test_speedup</th>\n",
       "      <th>train_mem_reduction</th>\n",
       "      <th>test_mem_reduction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>1.49945</td>\n",
       "      <td>2.31385</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>2.08250</td>\n",
       "      <td>1.24170</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.720024</td>\n",
       "      <td>1.863453</td>\n",
       "      <td>0.975472</td>\n",
       "      <td>0.703204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_mean_ms  test_mean_ms  mean_accuracy  train_speedup  \\\n",
       "model                                                                      \n",
       "QDA                  1.49945       2.31385       0.981481       1.000000   \n",
       "TensorizedQDA        2.08250       1.24170       0.953704       0.720024   \n",
       "\n",
       "               test_speedup  train_mem_reduction  test_mem_reduction  \n",
       "model                                                                 \n",
       "QDA                1.000000             1.000000            1.000000  \n",
       "TensorizedQDA      1.863453             0.975472            0.703204  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ[[\n",
    "    'train_mean_ms', 'test_mean_ms','mean_accuracy',\n",
    "    'train_speedup', 'test_speedup',\n",
    "    'train_mem_reduction', 'test_mem_reduction'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EF80Pck2RmaC"
   },
   "source": [
    "# Consigna QDA\n",
    "\n",
    "**Notación**: en general notamos\n",
    "\n",
    "* $k$ la cantidad de clases\n",
    "* $n$ la cantidad de observaciones\n",
    "* $p$ la cantidad de features/variables/predictores\n",
    "\n",
    "**Sugerencia:** combinaciones adecuadas de `transpose`, `stack`, `reshape` y, ocasionalmente, `flatten` y `diagonal` suele ser más que suficiente. Se recomienda **fuertemente* explorar la dimensionalidad de cada elemento antes de implementar las clases.\n",
    "\n",
    "## Tensorización\n",
    "\n",
    "En esta sección nos vamos a ocupar de hacer que el modelo sea más rápido para generar predicciones, observando que incurre en un doble `for` dado que predice en forma individual un escalar para cada observación, para cada clase. Paralelizar ambos vía tensorización suena como una gran vía de mejora de tiempos.\n",
    "\n",
    "### 1) Diferencias entre `QDA`y `TensorizedQDA`\n",
    "\n",
    "1. ¿Sobre qué paraleliza `TensorizedQDA`? ¿Sobre las $k$ clases, las $n$ observaciones a predecir, o ambas?\n",
    "2. Analizar los shapes de `tensor_inv_covs` y `tensor_means` y explicar paso a paso cómo es que `TensorizedQDA` llega a predecir lo mismo que `QDA`.\n",
    "\n",
    "### 2) Optimización\n",
    "\n",
    "Debido a la forma cuadrática de QDA, no se puede predecir para $n$ observaciones en una sola pasada (utilizar $X \\in \\mathbb{R}^{p \\times n}$ en vez de $x \\in \\mathbb{R}^p$) sin pasar por una matriz de $n \\times n$ en donde se computan todas las interacciones entre observaciones. Se puede acceder al resultado recuperando sólo la diagonal de dicha matriz, pero resulta ineficiente en tiempo y (especialmente) en memoria. Aún así, es *posible* que el modelo funcione más rápido.\n",
    "\n",
    "3. Implementar el modelo `FasterQDA` (se recomienda heredarlo de `TensorizedQDA`) de manera de eliminar el ciclo for en el método predict.\n",
    "4. Mostrar dónde aparece la mencionada matriz de $n \\times n$, donde $n$ es la cantidad de observaciones a predecir.\n",
    "5. Demostrar que\n",
    "$$\n",
    "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
    "$$ es decir, que se puede \"esquivar\" la matriz de $n \\times n$ usando matrices de $n \\times p$. También se puede usar, de forma equivalente,\n",
    "$$\n",
    "np.sum(A^T \\odot B, axis=0).T\n",
    "$$\n",
    "  queda a preferencia del alumno cuál usar.\n",
    "\n",
    "6. Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente en un nuevo modelo `EfficientQDA`.\n",
    "7. Comparar la performance de las 4 variantes de QDA implementadas hasta ahora (no Cholesky) ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?\n",
    "\n",
    "## Cholesky\n",
    "\n",
    "Hasta ahora todos los esfuerzos fueron enfocados en realizar una predicción más rápida. Los tiempos de entrenamiento (teóricos al menos) siguen siendo los mismos o hasta (minúsculamente) peores, dado que todas las mejoras siguen llamando al método `_fit_params` original de `QDA`.\n",
    "\n",
    "La descomposición/factorización de [Cholesky](https://en.wikipedia.org/wiki/Cholesky_decomposition#Statement) permite factorizar una matriz definida positiva $A = LL^T$ donde $L$ es una matriz triangular inferior. En particular, si bien se asume que $p \\ll n$, invertir la matriz de covarianzas $\\Sigma$ para cada clase impone un cuello de botella que podría alivianarse. Teniendo en cuenta que las matrices de covarianza son simétricas y salvo degeneración, definidas positivas, Cholesky como mínimo debería permitir invertir la matriz más rápido.\n",
    "\n",
    "*Nota: observar que calcular* $A^{-1}b$ *equivale a resolver el sistema* $Ax=b$.\n",
    "\n",
    "### 3) Diferencias entre implementaciones de `QDA_Chol`\n",
    "\n",
    "8. Si una matriz $A$ tiene fact. de Cholesky $A=LL^T$, expresar $A^{-1}$ en términos de $L$. ¿Cómo podría esto ser útil en la forma cuadrática de QDA?\n",
    "9. Explicar las diferencias entre `QDA_Chol1`y `QDA` y cómo `QDA_Chol1` llega, paso a paso, hasta las predicciones.\n",
    "10. ¿Cuáles son las diferencias entre `QDA_Chol1`, `QDA_Chol2` y `QDA_Chol3`?\n",
    "11. Comparar la performance de las 7 variantes de QDA implementadas hasta ahora ¿Qué se observa?¿Hay alguna de las implementaciones de `QDA_Chol` que sea claramente mejor que las demás?¿Alguna que sea peor?\n",
    "\n",
    "### 4) Optimización\n",
    "\n",
    "12. Implementar el modelo `TensorizedChol` paralelizando sobre clases/observaciones según corresponda. Se recomienda heredarlo de alguna de las implementaciones de `QDA_Chol`, aunque la elección de cuál de ellas queda a cargo del alumno según lo observado en los benchmarks de puntos anteriores.\n",
    "13. Implementar el modelo `EfficientChol` combinando los insights de `EfficientQDA` y `TensorizedChol`. Si se desea, se puede implementar `FasterChol` como ayuda, pero no se contempla para el punto.\n",
    "13. Comparar la performance de las 9 variantes de QDA implementadas ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwlW7sqwirdn"
   },
   "source": [
    "# Resolucion del TP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) ¿Sobre qué paraleliza TensorizedQDA? ¿Sobre las $k$ clases, las $n$ observaciones a predecir, o ambas?\n",
    "\n",
    "  TensorizedQDA paraleliza sobre ambas dimensiones: las $k$ clases y las $n$ observaciones a predecir.\n",
    "  En el modelo QDA original, la predicción implica un doble bucle: uno sobre las $k$ clases y otro sobre las $n$ observaciones, calculando el score discriminante para cada observación y cada clase de forma individual. Esto resulta en una complejidad computacional que escala con $O(n \\cdot k)$.\n",
    "  En cambio, TensorizedQDA utiliza operaciones tensorizadas (con matrices y vectores) para eliminar estos bucles explícitos, aprovechando el paralelismo implícito de las operaciones matriciales en bibliotecas como NumPy. Esto permite calcular los scores para todas las observaciones y todas las clases simultáneamente en una sola pasada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agustin:\n",
    "Tensoriza sobre las clases. En la versión previa el predict_log_conditional actuaba para cada clase individualmente. De esta forma, elimina el for loop de predict_one que operaba sobre las clases (enumerando el vector a_priori)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analizar los shapes de tensor_inv_covs y tensor_means y explicar paso a paso cómo es que TensorizedQDA llega a predecir lo mismo que QDA.\n",
    "\n",
    "- Shapes esperados:\n",
    "  - tensor_means: Shape $(k, p)$, donde $k$ es el número de clases y $p$ es el número de features. Representa los vectores de medias para cada clase.\n",
    "  - tensor_inv_covs: Shape $(k, p, p)$, donde cada \"capa\" $(p, p)$ es la matriz inversa de la covarianza para una clase específica.\n",
    "\n",
    "- Paso a paso en TensorizedQDA:\n",
    "  1. Entrada: Se recibe un conjunto de observaciones $X$ de shape $(n, p)$, donde $n$ es el número de observaciones a predecir.\n",
    "  2. Diferencia con las medias: Se calcula $X - \\mu_k$ para cada clase $k$. Esto se hace restando tensor_means (expandido a $(n, k, p)$ mediante broadcasting) a $X$ (expandido a $(n, k, p)$), resultando en una matriz de diferencias de shape $(n, k, p)$.\n",
    "  3. Forma cuadrática: Para cada clase $k$, se necesita calcular $(x_i - \\mu_k)^T \\Sigma_k^{-1} (x_i - \\mu_k)$ para cada observación $x_i$. En TensorizedQDA, esto se logra usando tensor_inv_covs $(k, p, p)$ y las diferencias $(n, k, p)$ mediante una operación matricial tensorizada (como einsum), produciendo un tensor de shape $(n, k)$ con los términos cuadráticos para todas las observaciones y clases.\n",
    "  4. Términos adicionales: Se suman los logaritmos de las probabilidades a priori (log_priors, shape $(k,)$) y los logaritmos de los determinantes de las covarianzas (calculados en el ajuste), ajustados para cada clase.\n",
    "  5. Predicción: Se toma el argmax a lo largo del eje de las clases (dimensión $k$) para obtener las predicciones de shape $(n,)$, igual que en QDA.\n",
    "    \n",
    "\n",
    "- Equivalencia: La tensorización no cambia la fórmula matemática del discriminante cuadrático, solo reorganiza los cálculos para evitar bucles explícitos, logrando el mismo resultado que QDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agustin:\n",
    "\n",
    "  - El shape de tensor_inv_covs es (cantidad de clases, cantidad de features, cantidad de features), siendo en este caso las clases 3 y las features 13, ya que para cada clase genera una matriz cuadrada de las covarianzas entre cada feature (siendo la diagonal la varianza propia de la feature). El shape de tensor_means es (cantidad de clases, cantidad de features, 1), ya que en este caso calcula una media para feature y para cada clase. El cambio con respecto a la versión previa es que antes cada matriz para cada clase se guardaba en una lista, y eran utilizadas dentro el for loop de predict_one, y ahora están agrupadas en un tensor. Para entender por qué llega al mismo resultado, debemos entender cómo funcionan las operaciones @ (np.matmul) y np.linalg.det(), cuando lo aplicamos sobre un tensor que consiste en k matrices apiladas. Lo que hacen ambas operaciones es aplicar la operación sobre las k matrices que están apiladas. Operación por operación ocurre lo siguiente:\n",
    "  - Con unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x, con shape de unbiased_x de (3,13,1) y tensor_inv_cov de (3,13,13): -unbiased_x.transpose(0,2,1) convierte el tensor a (3,1,13) -unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov multiplica el tensor de (3,1,13) con el de (3,13,13), donde internamente multiplica los 3 vectores de (1,13) por las 3 matrices de (13,13) y las vuelve a apilar. De esta forma, funciona igual que el for loop que lo hacía en forma separada para las 3, pero numpy lo hace en forma más eficiente al poder paralelizar la operación. El resultado es un tensor de (3,1,13) -Finalmente, el tensor de (3,1,13) es multiplicado por el tensor de unbiased_x resultando en un tensor de (3, 1, 1), siendo el segundo término de la log verosimilitud para cada una de las clases (al multiplicar por -0.5):\n",
    "\n",
    "$\\log f_j(x) = 12 \\log|\\Sigma_j| - 12(x-\\mu_j)T\\Sigma-1 j(x-\\mu_j) + C$\n",
    "\n",
    " log𝑓𝑗(𝑥)=−12log|Σ𝑗|−12(𝑥−𝜇𝑗)𝑇Σ−1𝑗(𝑥−𝜇𝑗)+𝐶$ \n",
    "  - np.log(LA.det(self.tensor_inv_cov)) la función det opera de la misma forma, ya que tensor_inv_cov es de (3,13,13) por lo que para cada una de las 3 clases, genera el determinante de las matrices de (13,13), esto multiplicado por 0.5 nos da el primer término de la ecuación\n",
    "\n",
    "  - Por último al sumar esto a la log_a_priori, tenemos las posterior, en formato tensor (3,1,1), y para predecir la clase se toma argmax La diferencia es que elimina el loop for de las clases, aplicando las operaciones mediante tensores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementar el modelo FasterQDA (se recomienda heredarlo de TensorizedQDA) de manera de eliminar el ciclo for en el método predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código elimina el bucle explícito sobre observaciones al usar einsum para calcular la forma cuadrática en una sola operación tensorizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterQDA(TensorizedQDA):\n",
    "    def predict(self, X):\n",
    "        m_obs = X.shape[1]\n",
    "        k_classes = len(self.log_a_priori)\n",
    "        \n",
    "        # Calcular las log-posteriori para todas las observaciones y clases\n",
    "        log_posteriori = np.empty((k_classes, m_obs))\n",
    "        \n",
    "        # Diferencia entre X y la media de la clase k\n",
    "        unbiased_X = X - self.tensor_means\n",
    "        # Esto genera una tensor de (n_clases, m_obs, m_obs)\n",
    "        inner_prod = unbiased_X.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_X\n",
    "        print(f\"Shape de inner_prod: {inner_prod.shape}\")\n",
    "        # Extraer solo la diagonal (log-probabilidad condicional para cada observación)\n",
    "        log_conditional = 0.5 * np.log(LA.det(self.tensor_inv_cov)).reshape(-1, 1) - 0.5 * np.diagonal(inner_prod, axis1=1, axis2=2)\n",
    "        # Sumar el log a priori\n",
    "        log_posteriori = self.log_a_priori.reshape(-1, 1) + log_conditional\n",
    "        \n",
    "        # Elegir la clase con máxima probabilidad log-posteriori\n",
    "        y_hat = np.argmax(log_posteriori, axis=0)\n",
    "        return y_hat.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterQDA_v2(TensorizedQDA):\n",
    "    #Implementación sin la matriz de nxn\n",
    "    def _predict_log_conditionals(self,X):\n",
    "        X = X.T # Pasar las observaciones a filas (m_obs, n_features)\n",
    "        X = X.reshape(X.shape[0], X.shape[1],1) # Agregar 3ra dimension (m_obs, n_features, 1)\n",
    "        X_expandido = np.expand_dims(X, axis=1)  # (m_obs, 1, n_features, 1)\n",
    "        X_expandido = np.repeat(X_expandido, repeats=len(self.log_a_priori), axis=1)  # len(self.log_a_priori) es la cantidad de clases. Replica el vector para cada clase (m_obs, k_clases, n_features, 1)\n",
    "        unbiased_X = X_expandido - self.tensor_means\n",
    "        inner_prod = unbiased_X.transpose(0,1,3,2) @ self.tensor_inv_cov @ unbiased_X\n",
    "        return 0.5*np.log(LA.det(self.tensor_inv_cov)) - 0.5 * inner_prod.squeeze()\n",
    "\n",
    "    def _predict_one(self, X):\n",
    "        # return the class that has maximum a posteriori probability\n",
    "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(X), axis =1)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # this is actually an individual prediction encased in a for-loop\n",
    "        y_hat = self._predict_one(X)\n",
    "\n",
    "        # return prediction as a row vector (matching y)\n",
    "        return y_hat.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n",
      "Shape de inner_prod: (3, 54, 54)\n",
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n",
      "Shape de inner_prod: (3, 54, 54)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d9e1c13fb649698e83ca6d9eadae39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FasterQDA (MEM):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n",
      "Shape de inner_prod: (3, 54, 54)\n",
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n",
      "Shape de inner_prod: (3, 54, 54)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e669c2d61ba74f0cabe5884d1d545408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FasterQDA (TIME):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n",
      "Shape de inner_prod: (3, 54, 54)\n",
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n",
      "Shape de inner_prod: (3, 54, 54)\n"
     ]
    }
   ],
   "source": [
    "b.bench(FasterQDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n",
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e575de116e4825be809f169a6813d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FasterQDA_v2 (MEM):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n",
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e405701b25a420799713ca3583b3ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FasterQDA_v2 (TIME):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n",
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n"
     ]
    }
   ],
   "source": [
    "b.bench(FasterQDA_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mean_ms</th>\n",
       "      <th>train_std_ms</th>\n",
       "      <th>test_mean_ms</th>\n",
       "      <th>test_std_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_mem_mean_mb</th>\n",
       "      <th>train_mem_std_mb</th>\n",
       "      <th>test_mem_mean_mb</th>\n",
       "      <th>test_mem_std_mb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>1.49945</td>\n",
       "      <td>0.15025</td>\n",
       "      <td>2.31385</td>\n",
       "      <td>0.03935</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.009177</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>2.08250</td>\n",
       "      <td>0.41690</td>\n",
       "      <td>1.24170</td>\n",
       "      <td>0.08850</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.019810</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.013051</td>\n",
       "      <td>0.000299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA</th>\n",
       "      <td>1.77050</td>\n",
       "      <td>0.18910</td>\n",
       "      <td>0.34120</td>\n",
       "      <td>0.02820</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.019398</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.112926</td>\n",
       "      <td>0.000930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA_v2</th>\n",
       "      <td>2.09650</td>\n",
       "      <td>0.32650</td>\n",
       "      <td>0.40690</td>\n",
       "      <td>0.19440</td>\n",
       "      <td>0.990741</td>\n",
       "      <td>0.019327</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.061795</td>\n",
       "      <td>0.000130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_mean_ms  train_std_ms  test_mean_ms  test_std_ms  \\\n",
       "model                                                                   \n",
       "QDA                  1.49945       0.15025       2.31385      0.03935   \n",
       "TensorizedQDA        2.08250       0.41690       1.24170      0.08850   \n",
       "FasterQDA            1.77050       0.18910       0.34120      0.02820   \n",
       "FasterQDA_v2         2.09650       0.32650       0.40690      0.19440   \n",
       "\n",
       "               mean_accuracy  train_mem_mean_mb  train_mem_std_mb  \\\n",
       "model                                                               \n",
       "QDA                 0.981481           0.019324          0.000088   \n",
       "TensorizedQDA       0.953704           0.019810          0.000597   \n",
       "FasterQDA           0.981481           0.019398          0.000175   \n",
       "FasterQDA_v2        0.990741           0.019327          0.000198   \n",
       "\n",
       "               test_mem_mean_mb  test_mem_std_mb  \n",
       "model                                             \n",
       "QDA                    0.009177         0.000900  \n",
       "TensorizedQDA          0.013051         0.000299  \n",
       "FasterQDA              0.112926         0.000930  \n",
       "FasterQDA_v2           0.061795         0.000130  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -- Gráficos de comparación\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Obtener el summary directamente desde el benchmark\n",
    "df_summary = b.summary()\n",
    "\n",
    "# Extraer columnas de interés\n",
    "tiempos = df_summary[\"test_mean_ms\"]\n",
    "accuracies = df_summary[\"mean_accuracy\"]\n",
    "\n",
    "# Crear gráfico combinado\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Barras: tiempo de predicción\n",
    "bars = ax1.bar(df_summary.index, tiempos, color='cornflowerblue', label=\"Tiempo de predicción (ms)\")\n",
    "ax1.set_ylabel(\"Tiempo de predicción (ms)\", color='steelblue')\n",
    "ax1.tick_params(axis='y', labelcolor='steelblue')\n",
    "\n",
    "# Línea: accuracy\n",
    "ax2 = ax1.twinx()\n",
    "line, = ax2.plot(df_summary.index, accuracies, color='seagreen', marker='o', linewidth=2, label=\"Accuracy\")\n",
    "ax2.set_ylabel(\"Accuracy\", color='seagreen')\n",
    "ax2.tick_params(axis='y', labelcolor='seagreen')\n",
    "\n",
    "# Agregar leyenda combinada (de ambas y-axes)\n",
    "fig.legend([bars, line], [\"Tiempo de predicción (ms)\", \"Accuracy\"], loc='upper center', bbox_to_anchor=(0.5, -0.08), ncol=2)\n",
    "\n",
    "# Título y formato\n",
    "plt.title(\"Comparación QDA: Tiempo de predicción vs Accuracy\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En resumen\n",
    "\n",
    "Se propusieron dos versiones (`FasterQDA` y `FasterQDA_v2`) que eliminan el ciclo `for` de predicción presente en la implementación clásica `QDA`.\n",
    "\n",
    "Ambas versiones utilizan **tensorización** para calcular los valores discriminantes cuadráticos para **todas las observaciones y clases en paralelo**, eliminando la necesidad de bucles explícitos y matrices intermedias costosas.\n",
    "\n",
    "- `FasterQDA` replica vectores de entrada y extrae las diagonales de las formas cuadráticas completas, logrando una ejecución más eficiente.\n",
    "- `FasterQDA_v2` refina aún más esta lógica, evitando por completo la construcción de matrices \\( n \\times n \\), utilizando `squeeze` y broadcasting controlado para optimizar tanto memoria como velocidad.\n",
    "\n",
    "#### Observaciones del gráfico:\n",
    "\n",
    "- **`QDA`** clásico es el más lento, con tiempos promedio de predicción significativamente mayores.\n",
    "- **`TensorizedQDA`** mejora la eficiencia respecto a `QDA`, pero su precisión (accuracy) es levemente inferior al resto.\n",
    "- **`FasterQDA`** mantiene la **misma accuracy** que `QDA`, pero con un **tiempo de predicción más de 20 veces menor**.\n",
    "- **`FasterQDA_v2`** se destaca como la mejor versión:  \n",
    "  - Es **la más precisa** del conjunto  \n",
    "  - Y una de las **más rápidas**, solo superada marginalmente por `FasterQDA`\n",
    "\n",
    "#### Conclusión final:\n",
    "\n",
    "Ambas versiones propuestas, `FasterQDA` y `FasterQDA_v2`, no solo eliminan los bucles, sino que logran **eficiencia computacional sin comprometer la precisión**.  \n",
    "En particular, **`FasterQDA_v2`** representa la **mejor combinación de performance y exactitud**, y constituye una solución altamente escalable para problemas de clasificación con QDA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Mostrar dónde aparece la mencionada matriz de $n \\times n$, donde $n$ es la cantidad de observaciones a predecir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  - La matriz de $n \\times n$ aparece en el término cuadrático si intentamos calcularlo directamente como $(X - M)^T \\Sigma^{-1} (X - M)$, donde $X$ es $(n, p)$ y $M$ es una matriz de medias repetidas para cada observación. En FasterQDA, esto ocurre en el tensor denominado inner_product (el tensor queda de la forma (k, n, n))\n",
    "    - diffs $(n, k, p)$ y tensor_inv_covs $(k, p, p)$ generan un producto intermedio que, si no se reduce inmediatamente, podría interpretarse como una interacción entre todas las $n$ observaciones para cada clase, pero np.diag colapsa esto a $(k, n)$ directamente, evitando materializar la matriz $n \\times n$ completa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo simple con 3 observaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz n x n de productos cruzados:\n",
      "[[ 1.  0. -1.]\n",
      " [ 0.  1. -1.]\n",
      " [-1. -1.  2.]]\n",
      "\n",
      "Diagonal (forma cuadrática por observación):\n",
      "[1. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "# -- Ejemplo breve con 3 observaciones --\n",
    "import numpy as np\n",
    "\n",
    "# Supongamos 3 observaciones con 2 features cada una\n",
    "X = np.array([\n",
    "    [1.0, 2.0],\n",
    "    [0.0, 1.0],\n",
    "    [2.0, 0.0]\n",
    "])  # shape (3, 2)\n",
    "\n",
    "# Media de la clase (fija para todas)\n",
    "mu = np.array([1.0, 1.0])  # shape (2,)\n",
    "\n",
    "# Inversa de la matriz de covarianza\n",
    "Sigma_inv = np.array([\n",
    "    [1.0, 0.0],\n",
    "    [0.0, 1.0]\n",
    "])  # identidad solo para simplificar\n",
    "\n",
    "# Paso 1: restamos la media (broadcasting)\n",
    "diff = X - mu  # shape (3, 2)\n",
    "\n",
    "# Paso 2: construimos la matriz completa (n x n)\n",
    "# Cada entrada [i, j] es: (x_i - mu)^T Σ^{-1} (x_j - mu)\n",
    "inner_product_full = diff @ Sigma_inv @ diff.T  # shape (3, 3)\n",
    "print(\"Matriz n x n de productos cruzados:\")\n",
    "print(inner_product_full)\n",
    "\n",
    "# Paso 3: extraemos solo la diagonal (lo que realmente usamos en QDA)\n",
    "inner_diag = np.diagonal(inner_product_full)  # shape (3,)\n",
    "print(\"\\nDiagonal (forma cuadrática por observación):\")\n",
    "print(inner_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretación del ejemplo\n",
    "\n",
    "- La matriz 3x3 tiene **productos cruzados innecesarios**.\n",
    "- La diagonal contiene lo importante:\n",
    "\n",
    "  $$\n",
    "  (x_i - \\mu)^T \\Sigma^{-1} (x_i - \\mu)\n",
    "  $$\n",
    "\n",
    "- En QDA no necesitamos toda la matriz. Solo usamos los valores diagonales que es la forma cuadrática por observación (score individual).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) **Demostrar que**  $\\text{diag}(A \\cdot B) = \\sum_{\\text{cols}} A \\odot B^T = \\texttt{np.sum}(A \\odot B^T, \\texttt{axis=1})$\n",
    "\n",
    "**Prueba:**\n",
    "\n",
    "Sea $ A \\in \\mathbb{R}^{n \\times k}$  y $B \\in \\mathbb{R}^{k \\times n}$.\n",
    "\n",
    "El producto $( A \\cdot B ) \\in \\mathbb{R}^{n \\times n} $, por lo que su diagonal:\n",
    "\n",
    "$$\n",
    "\\text{diag}(A \\cdot B)_i = \\sum_{j=1}^{k} A_{ij} B_{ji}\n",
    "$$\n",
    "\n",
    "Por definición de multiplicación de matrices:\n",
    "\n",
    "$$\n",
    "(A \\cdot B)_{ii} = \\sum_{j=1}^{k} A_{ij} \\cdot B_{ji}\n",
    "$$\n",
    "\n",
    "Por lo tanto:\n",
    "\n",
    "$$\n",
    "\\text{diag}(A \\cdot B)_i = \\sum_{j=1}^{k} A_{ij} \\cdot B_{ji}\n",
    "$$\n",
    "\n",
    "Dado que $( B^T \\in \\mathbb{R}^{n \\times k} )$, entonces $( A \\odot B^T \\in \\mathbb{R}^{n \\times k} )$\n",
    "\n",
    "El elemento en la posición \\( (i, j) \\) de \\( A \\odot B^T \\) es:\n",
    "\n",
    "$$\n",
    "(A \\odot B^T)_{ij} = A_{ij} \\cdot B^T_{ij} = A_{ij} \\cdot B_{ji}\n",
    "$$\n",
    "\n",
    "Entonces:\n",
    "\n",
    "$$\n",
    "\\sum_{\\text{cols}} (A \\odot B^T)_i = \\sum_{j=1}^{k} A_{ij} \\cdot B_{ji}\n",
    "$$\n",
    "\n",
    "Lo que equivale a la expresión para \\( \\text{diag}(A \\cdot B) \\)\n",
    "\n",
    "Por lo tanto:\n",
    "\n",
    "$$\n",
    "\\text{diag}(A \\cdot B) = \\texttt{np.sum}(A \\odot B^T, \\texttt{axis=1})\n",
    "$$\n",
    "\n",
    "En NumPy, dada una matriz `A` de dimensiones `(n, k)` y `B` con dimensiones `(k, n)`,  \n",
    "se puede calcular la diagonal de `A @ B` por medio de:\n",
    "\n",
    "```python\n",
    "np.sum(A * B.T, axis=1)\n",
    "\n",
    "Debido a que \n",
    "  - A * B.T es la multiplicación elemento a elemento\n",
    "  - np.sum(..., axis=1) suma en forma columnar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo FasterQDA de forma eficiente en un nuevo modelo EfficientQDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientQDA(TensorizedQDA):\n",
    "    def predict(self, X):\n",
    "        m_obs = X.shape[1]\n",
    "        k_classes = len(self.log_a_priori)\n",
    "        \n",
    "        # Calcular las log-posteriori para todas las observaciones y clases\n",
    "        log_posteriori = np.empty((k_classes, m_obs))\n",
    "        \n",
    "        X_expandido = np.expand_dims(X, axis=0)  # (m_obs, 1, n_features, 1)\n",
    "        X_expandido = np.repeat(X_expandido, repeats=len(self.log_a_priori), axis=0)\n",
    "        \n",
    "        # Diferencia entre X y la media de la clase k\n",
    "        unbiased_X = X - self.tensor_means\n",
    "        # Calcular inv_cov_k @ (X - mu_k)\n",
    "        cov_unbiased = self.tensor_inv_cov @ unbiased_X\n",
    "        # Usar la propiedad: suma de (X - mu_k)^T ⊙ (inv_cov_k @ (X - mu_k))\n",
    "        inner_prod = np.sum(unbiased_X.T * cov_unbiased.T, axis=1)\n",
    "        # Extraer solo la diagonal (log-probabilidad condicional para cada observación)\n",
    "        log_conditional = 0.5 * np.log(LA.det(self.tensor_inv_cov)).reshape(-1, 1) - 0.5 * inner_prod.T\n",
    "        # Sumar el log a priori\n",
    "        log_posteriori = self.log_a_priori.reshape(-1, 1) + log_conditional\n",
    "        \n",
    "        # Elegir la clase con máxima probabilidad log-posteriori\n",
    "        y_hat = np.argmax(log_posteriori, axis=0)\n",
    "        return y_hat.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n",
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f05b373a44f48e7ae59d3b099c6c364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EfficientQDA (MEM):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n",
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0257453160422aab794b21e78f995f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EfficientQDA (TIME):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n",
      "X shape=  (13, 124)\n",
      "y shape=  (1, 124)\n"
     ]
    }
   ],
   "source": [
    "b.bench(EfficientQDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mean_ms</th>\n",
       "      <th>train_std_ms</th>\n",
       "      <th>test_mean_ms</th>\n",
       "      <th>test_std_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_mem_mean_mb</th>\n",
       "      <th>train_mem_std_mb</th>\n",
       "      <th>test_mem_mean_mb</th>\n",
       "      <th>test_mem_std_mb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>1.49945</td>\n",
       "      <td>0.15025</td>\n",
       "      <td>2.31385</td>\n",
       "      <td>0.03935</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.009177</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>2.08250</td>\n",
       "      <td>0.41690</td>\n",
       "      <td>1.24170</td>\n",
       "      <td>0.08850</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.019810</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.013051</td>\n",
       "      <td>0.000299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA</th>\n",
       "      <td>1.77050</td>\n",
       "      <td>0.18910</td>\n",
       "      <td>0.34120</td>\n",
       "      <td>0.02820</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.019398</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.112926</td>\n",
       "      <td>0.000930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA_v2</th>\n",
       "      <td>2.09650</td>\n",
       "      <td>0.32650</td>\n",
       "      <td>0.40690</td>\n",
       "      <td>0.19440</td>\n",
       "      <td>0.990741</td>\n",
       "      <td>0.019327</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.061795</td>\n",
       "      <td>0.000130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientQDA</th>\n",
       "      <td>1.48215</td>\n",
       "      <td>0.12545</td>\n",
       "      <td>0.16705</td>\n",
       "      <td>0.00255</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.019044</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.095157</td>\n",
       "      <td>0.000393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_mean_ms  train_std_ms  test_mean_ms  test_std_ms  \\\n",
       "model                                                                   \n",
       "QDA                  1.49945       0.15025       2.31385      0.03935   \n",
       "TensorizedQDA        2.08250       0.41690       1.24170      0.08850   \n",
       "FasterQDA            1.77050       0.18910       0.34120      0.02820   \n",
       "FasterQDA_v2         2.09650       0.32650       0.40690      0.19440   \n",
       "EfficientQDA         1.48215       0.12545       0.16705      0.00255   \n",
       "\n",
       "               mean_accuracy  train_mem_mean_mb  train_mem_std_mb  \\\n",
       "model                                                               \n",
       "QDA                 0.981481           0.019324          0.000088   \n",
       "TensorizedQDA       0.953704           0.019810          0.000597   \n",
       "FasterQDA           0.981481           0.019398          0.000175   \n",
       "FasterQDA_v2        0.990741           0.019327          0.000198   \n",
       "EfficientQDA        0.981481           0.019044          0.000603   \n",
       "\n",
       "               test_mem_mean_mb  test_mem_std_mb  \n",
       "model                                             \n",
       "QDA                    0.009177         0.000900  \n",
       "TensorizedQDA          0.013051         0.000299  \n",
       "FasterQDA              0.112926         0.000930  \n",
       "FasterQDA_v2           0.061795         0.000130  \n",
       "EfficientQDA           0.095157         0.000393  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Comparar la performance de las 4 variantes de QDA implementadas hasta ahora (no Cholesky) ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Aspecto                  | QDA                              | TensorizedQDA                    | FasterQDA_v2                     | EfficientQDA                     |\n",
    "|--------------------------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|\n",
    "| **Bucle sobre observaciones** | Sí $(n)$                    | Sí $(n)$                       | No                               | No                               |\n",
    "| **Bucle sobre clases**   | Sí (\\(k\\))                      | No                              | No                               | No                               |\n",
    "| **Uso de tensorización** | No                              | Sí (sobre clases)               | Sí (sobre clases y observaciones) | Sí (sobre clases y observaciones) |\n",
    "| **Complejidad**          | $O(n \\cdot k \\cdot p^2)$      | $O(n \\cdot p^2)$              | $O(p^2 \\cdot k)$               | $O(p^2 \\cdot k)$               |\n",
    "| **Predicción en lote**   | No                              | No                              | Sí                               | Sí                               |\n",
    "| **Matriz $(n \\times n)$** | No evitada                     | No evitada                     | Evitada                          | Evitada                          |\n",
    "| **Cálculo de log-determinantes** | Correcto (por clase)         | Correcto (por clase)            | Incorrecto (tensor 3D); necesita corrección | Incorrecto (tensor 3D); necesita corrección |\n",
    "| **Estrategia de optimización** | Bucles explícitos             | Tensorización parcial (clases)  | Tensorización completa, términos cuadráticos con `@` | Tensorización completa, suma de productos para términos cuadráticos |\n",
    "\n",
    "**Notas:**\n",
    "- \\(n\\): número de observaciones, \\(k\\): número de clases, \\(p\\): número de características.\n",
    "- **QDA**: Implementación base con bucles sobre observaciones y clases, menos eficiente para grandes \\(n\\) y \\(k\\).\n",
    "- **TensorizedQDA**: Elimina el bucle sobre clases usando tensorización, pero aún itera sobre observaciones.\n",
    "- **FasterQDA_v2**: Procesa todas las observaciones y clases simultáneamente con operaciones matriciales batcheadas, pero requiere corregir el cálculo de `log(LA.det(self.tensor_inv_cov))` almacenando `self.log_dets` en `_fit_params`.\n",
    "- **EfficientQDA**: Similar a `FasterQDA_v2`, usa tensorización completa y optimiza términos cuadráticos con suma de productos, pero también necesita corregir el cálculo de log-determinantes.\n",
    "- Para ambos, `FasterQDA_v2` y `EfficientQDA`, se recomienda implementar:\n",
    "  ```python\n",
    "  def _fit_params(self, X, y):\n",
    "      super()._fit_params(X, y)\n",
    "      self.log_dets = np.array([np.log(np.linalg.det(self.covs_[k])) for k in range(len(self.log_a_priori))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Si una matriz $A$ tiene fact. de Cholesky $A=LL^T$, expresar $A^{-1}$ en términos de $L$. ¿Cómo podría esto ser útil en la forma cuadrática de QDA?\n",
    "\n",
    "La matriz $A^{-1}=(LL^T)^{-1}=L^{-T}L^{-1}$\n",
    "\n",
    "Para aplicar Cholesky, requiero que la matriz sea simétrica y definida positiva. La primera condición se cumple siempre para la matriz de covarianza, mientras que la segunda siempre que no haya dependencia lineal exacta entre variables. Suponiendo que las condiciones se cumplen, puedo expresar:\n",
    "\n",
    "$\\Sigma_j^{-1} = L_j^{-T}L_j^{-1}$\n",
    "\n",
    "Esto permite expresar la inversa de la matriz de covarianzas en función de las matrices $L^{-1}$\n",
    "\n",
    "Recordando que teníamos:\n",
    "$$\n",
    "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "\n",
    "*   El primer término es:\n",
    "    $$ -\\frac{1}{2}\\log |\\Sigma_j|= -\\frac{1}{2}\\log |L_jL_j^T|=\\frac{1}{2}\\log (|L_j||L_j^T|)=-\\frac{1}{2}\\log (|L_j||L_j|)=\\frac{1}{2}\\log (|L_j|^2)=-\\log (|L_j|)$$\n",
    "    \n",
    "    Y el determinante de una matriz diagonal es el producto de los valores de la diagonal:\n",
    "    $$|L_j|=\\prod_{i=1}^{n} L_{ii}$$\n",
    "\n",
    "    En la aplicación en código, se calcula el determinante sobre la inversa, por lo que cancela el signo negativo\n",
    "\n",
    "*   Reemplazando el segundo término de $\\log{f_j(x)}$ con esta definición:\n",
    "    $$\\frac{1}{2}(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)=\\frac{1}{2}(x-\\mu_j)^T  L_j^{-T}L_j^{-1} (x- \\mu_j)$$\n",
    "\n",
    "    Y si defino $y=L^{-1} (x- \\mu_j)$, obtengo que:\n",
    "    $$\\frac{1}{2}(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)=\\frac{1}{2}(x-\\mu_j)^T  L_j^{-T}L_j^{-1} (x- \\mu_j)=\\frac{1}{2}y^Ty=\\frac{1}{2}\\|y\\|^2$$\n",
    "\n",
    "Esto significa que puedo obtener el ambos términos de $\\log{f_j(x)}$ calculando únicamente la matriz $L_j^{-1}$, sin necesidad de reconstruir la inversa completa $\\Sigma_j^{-1}$. Y como $L_j$ es una matriz triangular, invertirla es más barato computacionalmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Explicar las diferencias entre `QDA_Chol1`y `QDA` y cómo `QDA_Chol1` llega, paso a paso, hasta las predicciones.\n",
    "\n",
    "De acuerdo a lo demostrado en el punto anterior, podemos aprovechar la factorización de Cholesky para expresar $\\Sigma_j^{-1}$ como L_j^{-T}L_j^{-1}$, y esto nos permite expresar la log probabilidad condicional (sin el término constante), como:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} = \\frac{1}{2}\\log |\\Sigma_j^{-1}| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) = \\log (|L_j^{-1}|) - \\frac{1}{2}\\|y\\|^2\n",
    "$$\n",
    "$$\n",
    "\\log{f_j(x)} = \\log (|L_j^{-1}|) - \\frac{1}{2}\\|y\\|^2\n",
    "$$\n",
    "$$\n",
    "\\log{f_j(x)} = \\log (\\prod_{i=1}^{n} L_{ii}^{-1}) - \\frac{1}{2}\\|L^{-1} (x- \\mu_j)\\|^2\n",
    "$$\n",
    "\n",
    "A nivel de código, dentro de la función `_fit_params` obtiene la matriz $L$ para cada clase y calcula la inversa con la siguiente línea:<br>\n",
    "    ```self.L_invs = [\n",
    "            inv(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True))\n",
    "            for idx in range(len(self.log_a_priori))\n",
    "        ]\n",
    "    ```\n",
    "\n",
    "Luego dentro de la función `_predict_log_conditional`, calcula el vector $y$:<br>\n",
    "    ```y = L_inv @ unbiased_x```\n",
    "\n",
    "Y finalmente obtiene la log probabilidad condicional de acuerdo a la fórmula expresada al inicio con la siguiente línea:<br>\n",
    "    ```np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()```\n",
    "\n",
    "Luego de lo cual lo utiliza para las predicciones con a la misma función que utilizaba `QDA`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. ¿Cuáles son las diferencias entre `QDA_Chol1`, `QDA_Chol2` y `QDA_Chol3`?\n",
    "Las principales diferencias de  `QDA_Chol2` y `QDA_Chol3` con respecto a la versión `QDA_Chol1` explicada en la pregunta anterior son:\n",
    "* `QDA_Chol2`: factoriza la matriz $L$ de Cholesky pero no la invierte. Recordemos que la log probabilidad condicional en función de $L$ se expresaba de la siguiente forma:\n",
    "$$\n",
    "\\log{f_j(x)} = \\log (\\prod_{i=1}^{n} L_{ii}^{-1}) - \\frac{1}{2}\\|L^{-1} (x- \\mu_j)\\|^2\n",
    "$$\n",
    "Para calcular el primer término se puede utilizar directamente $L$ sin invertir, con el único cambio de que se agrega el signo negativo (ya que esto parte de si utilizamos $|\\Sigma_j^{-1}|$ o $|\\Sigma_j|$ en la ecuación original).<br>\n",
    "No obstante, el término $y=L^{-1} (x- \\mu_j)$ sí requeriría invertir la matriz $L$. La forma en que lo resuelve es utilizando la función `solve_triangular`, que resuelve la ecuación $Ax = b$ y obtiene $x$ para matrices triangulares utilizando sustitución en lugar de invertir.<br>\n",
    "Aplicada a $y$, lo que se termina obteniendo es:\n",
    "$$\n",
    "Ly=(x- \\mu_j) \\rightarrow y=L^{-1} (x- \\mu_j)\n",
    "$$\n",
    "De esta forma, logra obtener los términos de log probabilidad condicional sin invertir la matriz $L$\n",
    "* `QDA_Chol3`: es similar `QDA_Chol1` en que calcula la inversa de $L$, pero para hacerlo utiliza la función `dtrtri` en lugar de las funciones de Numpy. Esta función es de bajo nivel e invierte específicamente matrices triangulares, por lo que lo hace de forma más eficiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Comparar la performance de las 7 variantes de QDA implementadas hasta ahora ¿Qué se observa?¿Hay alguna de las implementaciones de `QDA_Chol` que sea claramente mejor que las demás?¿Alguna que sea peor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f259a517ac4056aacd5113abff3758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA_Chol1 (MEM):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923ac2a1f6df4fa48f7b4dce2f153127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA_Chol1 (TIME):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b.bench(QDA_Chol1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615ed2524174451d86170f4728529c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA_Chol2 (MEM):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d9b030487d479b9cce88d80f032252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA_Chol2 (TIME):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b.bench(QDA_Chol2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1185a54e88304d618ee303b4a6432785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA_Chol3 (MEM):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f26b71b9a7422f96f3566d881ee976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QDA_Chol3 (TIME):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b.bench(QDA_Chol3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mean_ms</th>\n",
       "      <th>train_std_ms</th>\n",
       "      <th>test_mean_ms</th>\n",
       "      <th>test_std_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_mem_mean_mb</th>\n",
       "      <th>train_mem_std_mb</th>\n",
       "      <th>test_mem_mean_mb</th>\n",
       "      <th>test_mem_std_mb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>1.49945</td>\n",
       "      <td>0.15025</td>\n",
       "      <td>2.31385</td>\n",
       "      <td>0.03935</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.009177</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>2.08250</td>\n",
       "      <td>0.41690</td>\n",
       "      <td>1.24170</td>\n",
       "      <td>0.08850</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.019810</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.013051</td>\n",
       "      <td>0.000299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA</th>\n",
       "      <td>1.77050</td>\n",
       "      <td>0.18910</td>\n",
       "      <td>0.34120</td>\n",
       "      <td>0.02820</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.019398</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.112926</td>\n",
       "      <td>0.000930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA_v2</th>\n",
       "      <td>2.09650</td>\n",
       "      <td>0.32650</td>\n",
       "      <td>0.40690</td>\n",
       "      <td>0.19440</td>\n",
       "      <td>0.990741</td>\n",
       "      <td>0.019327</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.061795</td>\n",
       "      <td>0.000130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientQDA</th>\n",
       "      <td>1.48215</td>\n",
       "      <td>0.12545</td>\n",
       "      <td>0.16705</td>\n",
       "      <td>0.00255</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.019044</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.095157</td>\n",
       "      <td>0.000393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol1</th>\n",
       "      <td>1.69820</td>\n",
       "      <td>0.06340</td>\n",
       "      <td>1.42175</td>\n",
       "      <td>0.06975</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.019325</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.008522</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol2</th>\n",
       "      <td>0.52210</td>\n",
       "      <td>0.12060</td>\n",
       "      <td>3.02705</td>\n",
       "      <td>0.01935</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.019980</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.008348</td>\n",
       "      <td>0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol3</th>\n",
       "      <td>0.44170</td>\n",
       "      <td>0.07890</td>\n",
       "      <td>1.33340</td>\n",
       "      <td>0.00850</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.018620</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.008645</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_mean_ms  train_std_ms  test_mean_ms  test_std_ms  \\\n",
       "model                                                                   \n",
       "QDA                  1.49945       0.15025       2.31385      0.03935   \n",
       "TensorizedQDA        2.08250       0.41690       1.24170      0.08850   \n",
       "FasterQDA            1.77050       0.18910       0.34120      0.02820   \n",
       "FasterQDA_v2         2.09650       0.32650       0.40690      0.19440   \n",
       "EfficientQDA         1.48215       0.12545       0.16705      0.00255   \n",
       "QDA_Chol1            1.69820       0.06340       1.42175      0.06975   \n",
       "QDA_Chol2            0.52210       0.12060       3.02705      0.01935   \n",
       "QDA_Chol3            0.44170       0.07890       1.33340      0.00850   \n",
       "\n",
       "               mean_accuracy  train_mem_mean_mb  train_mem_std_mb  \\\n",
       "model                                                               \n",
       "QDA                 0.981481           0.019324          0.000088   \n",
       "TensorizedQDA       0.953704           0.019810          0.000597   \n",
       "FasterQDA           0.981481           0.019398          0.000175   \n",
       "FasterQDA_v2        0.990741           0.019327          0.000198   \n",
       "EfficientQDA        0.981481           0.019044          0.000603   \n",
       "QDA_Chol1           0.981481           0.019325          0.000252   \n",
       "QDA_Chol2           1.000000           0.019980          0.000253   \n",
       "QDA_Chol3           0.972222           0.018620          0.000079   \n",
       "\n",
       "               test_mem_mean_mb  test_mem_std_mb  \n",
       "model                                             \n",
       "QDA                    0.009177         0.000900  \n",
       "TensorizedQDA          0.013051         0.000299  \n",
       "FasterQDA              0.112926         0.000930  \n",
       "FasterQDA_v2           0.061795         0.000130  \n",
       "EfficientQDA           0.095157         0.000393  \n",
       "QDA_Chol1              0.008522         0.000046  \n",
       "QDA_Chol2              0.008348         0.000079  \n",
       "QDA_Chol3              0.008645         0.000061  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que:\n",
    "* `QDA_Chol1`: el tiempo promedio de entrenamiento de no resulta mejor a las versiones de QDA que no utilizaban Cholesky, siendo incluso superior a QDA. Esto podría deberse a que ahora realiza dos pasos (factorización de Cholesky, e invertir $L$), por lo que no mejora los tiempos.\n",
    "* `QDA_Chol2`: se observa una mejora significativa en el tiempo promedio de entrenamiento respecto a las versiones anteriores, lo cual se debe a que ahora no se invierte la matriz $L$. No obstante los tiempos promedios de test resultan muy superiores al resto, debido a que ahora cada vez que se realiza una predicción utiliza la función solve_triangular\n",
    "* `QDA_Chol3`: esta implementación sería la óptima, ya que logra una mejora en los tiempos promedios de entrenamiento, sin empeorar los tiempos de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Implementar el modelo `TensorizedChol` paralelizando sobre clases/observaciones según corresponda. Se recomienda heredarlo de alguna de las implementaciones de `QDA_Chol`, aunque la elección de cuál de ellas queda a cargo del alumno según lo observado en los benchmarks de puntos anteriores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que la implementación óptima de Cholesky según los benchmarks es `QDA_Chol3`, utilizamos ésta como base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorizedChol(QDA_Chol3):\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        # ask plain QDA to fit params\n",
    "        super()._fit_params(X,y)\n",
    "\n",
    "        # stack onto new dimension\n",
    "        self.tensor_L_invs = np.stack(self.L_invs)\n",
    "        self.tensor_means = np.stack(self.means)\n",
    "\n",
    "    def _predict_log_conditionals(self,x):\n",
    "        unbiased_x = x - self.tensor_means\n",
    "        y = self.tensor_L_invs @ unbiased_x\n",
    "        return -np.log(self.tensor_L_invs.diagonal(axis1=1, axis2=2).prod(axis=1)).reshape(-1,1) -0.5 * (y**2).sum(axis=1)\n",
    "\n",
    "    def _predict_one(self, x):\n",
    "        # return the class that has maximum a posteriori probability\n",
    "        test = self.log_a_priori + self._predict_log_conditionals(x)\n",
    "        return np.argmax(self.log_a_priori.reshape(-1,1) + self._predict_log_conditionals(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb10d053b0b44e88bd171a4529b918a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TensorizedChol (MEM):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e90275aa5644fa83bb6eeecd4a994a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TensorizedChol (TIME):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b.bench(TensorizedChol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mean_ms</th>\n",
       "      <th>train_std_ms</th>\n",
       "      <th>test_mean_ms</th>\n",
       "      <th>test_std_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_mem_mean_mb</th>\n",
       "      <th>train_mem_std_mb</th>\n",
       "      <th>test_mem_mean_mb</th>\n",
       "      <th>test_mem_std_mb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>1.49945</td>\n",
       "      <td>0.15025</td>\n",
       "      <td>2.31385</td>\n",
       "      <td>0.03935</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.009177</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>2.08250</td>\n",
       "      <td>0.41690</td>\n",
       "      <td>1.24170</td>\n",
       "      <td>0.08850</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.019810</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.013051</td>\n",
       "      <td>0.000299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA</th>\n",
       "      <td>1.77050</td>\n",
       "      <td>0.18910</td>\n",
       "      <td>0.34120</td>\n",
       "      <td>0.02820</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.019398</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.112926</td>\n",
       "      <td>0.000930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA_v2</th>\n",
       "      <td>2.09650</td>\n",
       "      <td>0.32650</td>\n",
       "      <td>0.40690</td>\n",
       "      <td>0.19440</td>\n",
       "      <td>0.990741</td>\n",
       "      <td>0.019327</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.061795</td>\n",
       "      <td>0.000130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientQDA</th>\n",
       "      <td>1.48215</td>\n",
       "      <td>0.12545</td>\n",
       "      <td>0.16705</td>\n",
       "      <td>0.00255</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.019044</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.095157</td>\n",
       "      <td>0.000393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol1</th>\n",
       "      <td>1.69820</td>\n",
       "      <td>0.06340</td>\n",
       "      <td>1.42175</td>\n",
       "      <td>0.06975</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.019325</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.008522</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol2</th>\n",
       "      <td>0.52210</td>\n",
       "      <td>0.12060</td>\n",
       "      <td>3.02705</td>\n",
       "      <td>0.01935</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.019980</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.008348</td>\n",
       "      <td>0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol3</th>\n",
       "      <td>0.44170</td>\n",
       "      <td>0.07890</td>\n",
       "      <td>1.33340</td>\n",
       "      <td>0.00850</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.018620</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.008645</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedChol</th>\n",
       "      <td>0.59045</td>\n",
       "      <td>0.06115</td>\n",
       "      <td>1.37300</td>\n",
       "      <td>0.00480</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.018116</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.013446</td>\n",
       "      <td>0.000081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                train_mean_ms  train_std_ms  test_mean_ms  test_std_ms  \\\n",
       "model                                                                    \n",
       "QDA                   1.49945       0.15025       2.31385      0.03935   \n",
       "TensorizedQDA         2.08250       0.41690       1.24170      0.08850   \n",
       "FasterQDA             1.77050       0.18910       0.34120      0.02820   \n",
       "FasterQDA_v2          2.09650       0.32650       0.40690      0.19440   \n",
       "EfficientQDA          1.48215       0.12545       0.16705      0.00255   \n",
       "QDA_Chol1             1.69820       0.06340       1.42175      0.06975   \n",
       "QDA_Chol2             0.52210       0.12060       3.02705      0.01935   \n",
       "QDA_Chol3             0.44170       0.07890       1.33340      0.00850   \n",
       "TensorizedChol        0.59045       0.06115       1.37300      0.00480   \n",
       "\n",
       "                mean_accuracy  train_mem_mean_mb  train_mem_std_mb  \\\n",
       "model                                                                \n",
       "QDA                  0.981481           0.019324          0.000088   \n",
       "TensorizedQDA        0.953704           0.019810          0.000597   \n",
       "FasterQDA            0.981481           0.019398          0.000175   \n",
       "FasterQDA_v2         0.990741           0.019327          0.000198   \n",
       "EfficientQDA         0.981481           0.019044          0.000603   \n",
       "QDA_Chol1            0.981481           0.019325          0.000252   \n",
       "QDA_Chol2            1.000000           0.019980          0.000253   \n",
       "QDA_Chol3            0.972222           0.018620          0.000079   \n",
       "TensorizedChol       0.925926           0.018116          0.001202   \n",
       "\n",
       "                test_mem_mean_mb  test_mem_std_mb  \n",
       "model                                              \n",
       "QDA                     0.009177         0.000900  \n",
       "TensorizedQDA           0.013051         0.000299  \n",
       "FasterQDA               0.112926         0.000930  \n",
       "FasterQDA_v2            0.061795         0.000130  \n",
       "EfficientQDA            0.095157         0.000393  \n",
       "QDA_Chol1               0.008522         0.000046  \n",
       "QDA_Chol2               0.008348         0.000079  \n",
       "QDA_Chol3               0.008645         0.000061  \n",
       "TensorizedChol          0.013446         0.000081  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que la implementación de `TensorizedChol` mantiene el bajo tiempo de entrenamiento `QDA_Chol3` y mejora el tiempo de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Implementar el modelo `EfficientChol` combinando los insights de `EfficientQDA` y `TensorizedChol`. Si se desea, se puede implementar `FasterChol` como ayuda, pero no se contempla para el punto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientChol(TensorizedChol):\n",
    "    def predict(self, X):\n",
    "        m_obs = X.shape[1]\n",
    "        k_classes = len(self.log_a_priori)\n",
    "        \n",
    "        # Calcular las log-posteriori para todas las observaciones y clases\n",
    "        log_posteriori = np.empty((k_classes, m_obs))\n",
    "        \n",
    "        X_expandido = np.expand_dims(X, axis=0)  # (m_obs, 1, n_features, 1)\n",
    "        X_expandido = np.repeat(X_expandido, repeats=len(self.log_a_priori), axis=0)\n",
    "        \n",
    "        # Diferencia entre X y la media de la clase k\n",
    "        unbiased_X = X - self.tensor_means\n",
    "        # Calcular tensor_L_invs @ (X - mu_k)\n",
    "        y = self.tensor_L_invs @ unbiased_X\n",
    "        # Extraer solo la diagonal (log-probabilidad condicional para cada observación)\n",
    "        log_conditional = -np.log(self.tensor_L_invs.diagonal(axis1=1, axis2=2).prod(axis=1)).reshape(-1, 1) -0.5 * (y**2).sum(axis=1)\n",
    "        # Sumar el log a priori\n",
    "        log_posteriori = self.log_a_priori.reshape(-1, 1) + log_conditional\n",
    "        \n",
    "        # Elegir la clase con máxima probabilidad log-posteriori\n",
    "        y_hat = np.argmax(log_posteriori, axis=0)\n",
    "        return y_hat.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcccd93716604007938831de9d375a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EfficientChol (MEM):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143c70d2ff074aa2b8ad04837df41f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EfficientChol (TIME):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b.bench(EfficientChol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Comparar la performance de las 9 variantes de QDA implementadas ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos el summary con todas las variantes de QDA implementadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mean_ms</th>\n",
       "      <th>train_std_ms</th>\n",
       "      <th>test_mean_ms</th>\n",
       "      <th>test_std_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_mem_mean_mb</th>\n",
       "      <th>train_mem_std_mb</th>\n",
       "      <th>test_mem_mean_mb</th>\n",
       "      <th>test_mem_std_mb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>1.49945</td>\n",
       "      <td>0.15025</td>\n",
       "      <td>2.31385</td>\n",
       "      <td>0.03935</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.009177</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>2.08250</td>\n",
       "      <td>0.41690</td>\n",
       "      <td>1.24170</td>\n",
       "      <td>0.08850</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.019810</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.013051</td>\n",
       "      <td>0.000299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA</th>\n",
       "      <td>1.77050</td>\n",
       "      <td>0.18910</td>\n",
       "      <td>0.34120</td>\n",
       "      <td>0.02820</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.019398</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.112926</td>\n",
       "      <td>0.000930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FasterQDA_v2</th>\n",
       "      <td>2.09650</td>\n",
       "      <td>0.32650</td>\n",
       "      <td>0.40690</td>\n",
       "      <td>0.19440</td>\n",
       "      <td>0.990741</td>\n",
       "      <td>0.019327</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.061795</td>\n",
       "      <td>0.000130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientQDA</th>\n",
       "      <td>1.48215</td>\n",
       "      <td>0.12545</td>\n",
       "      <td>0.16705</td>\n",
       "      <td>0.00255</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.019044</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.095157</td>\n",
       "      <td>0.000393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol1</th>\n",
       "      <td>1.69820</td>\n",
       "      <td>0.06340</td>\n",
       "      <td>1.42175</td>\n",
       "      <td>0.06975</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.019325</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.008522</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol2</th>\n",
       "      <td>0.52210</td>\n",
       "      <td>0.12060</td>\n",
       "      <td>3.02705</td>\n",
       "      <td>0.01935</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.019980</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.008348</td>\n",
       "      <td>0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA_Chol3</th>\n",
       "      <td>0.44170</td>\n",
       "      <td>0.07890</td>\n",
       "      <td>1.33340</td>\n",
       "      <td>0.00850</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.018620</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.008645</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedChol</th>\n",
       "      <td>0.59045</td>\n",
       "      <td>0.06115</td>\n",
       "      <td>1.37300</td>\n",
       "      <td>0.00480</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.018116</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.013446</td>\n",
       "      <td>0.000081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EfficientChol</th>\n",
       "      <td>0.68740</td>\n",
       "      <td>0.23940</td>\n",
       "      <td>0.11455</td>\n",
       "      <td>0.02475</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.017429</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.094385</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                train_mean_ms  train_std_ms  test_mean_ms  test_std_ms  \\\n",
       "model                                                                    \n",
       "QDA                   1.49945       0.15025       2.31385      0.03935   \n",
       "TensorizedQDA         2.08250       0.41690       1.24170      0.08850   \n",
       "FasterQDA             1.77050       0.18910       0.34120      0.02820   \n",
       "FasterQDA_v2          2.09650       0.32650       0.40690      0.19440   \n",
       "EfficientQDA          1.48215       0.12545       0.16705      0.00255   \n",
       "QDA_Chol1             1.69820       0.06340       1.42175      0.06975   \n",
       "QDA_Chol2             0.52210       0.12060       3.02705      0.01935   \n",
       "QDA_Chol3             0.44170       0.07890       1.33340      0.00850   \n",
       "TensorizedChol        0.59045       0.06115       1.37300      0.00480   \n",
       "EfficientChol         0.68740       0.23940       0.11455      0.02475   \n",
       "\n",
       "                mean_accuracy  train_mem_mean_mb  train_mem_std_mb  \\\n",
       "model                                                                \n",
       "QDA                  0.981481           0.019324          0.000088   \n",
       "TensorizedQDA        0.953704           0.019810          0.000597   \n",
       "FasterQDA            0.981481           0.019398          0.000175   \n",
       "FasterQDA_v2         0.990741           0.019327          0.000198   \n",
       "EfficientQDA         0.981481           0.019044          0.000603   \n",
       "QDA_Chol1            0.981481           0.019325          0.000252   \n",
       "QDA_Chol2            1.000000           0.019980          0.000253   \n",
       "QDA_Chol3            0.972222           0.018620          0.000079   \n",
       "TensorizedChol       0.925926           0.018116          0.001202   \n",
       "EfficientChol        0.944444           0.017429          0.000508   \n",
       "\n",
       "                test_mem_mean_mb  test_mem_std_mb  \n",
       "model                                              \n",
       "QDA                     0.009177         0.000900  \n",
       "TensorizedQDA           0.013051         0.000299  \n",
       "FasterQDA               0.112926         0.000930  \n",
       "FasterQDA_v2            0.061795         0.000130  \n",
       "EfficientQDA            0.095157         0.000393  \n",
       "QDA_Chol1               0.008522         0.000046  \n",
       "QDA_Chol2               0.008348         0.000079  \n",
       "QDA_Chol3               0.008645         0.000061  \n",
       "TensorizedChol          0.013446         0.000081  \n",
       "EfficientChol           0.094385         0.000093  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La que combina la mejor eficiencia entre tiempos de entrenamiento y tiempos de testeo es `EfficientChol`. Esto es lo esperado, ya que combina las eficiencias ganadas en la etapa de entrenamiento utilizando Cholesky en lugar de invertir la matriz de covarianzas directamente, con la eficiencia en entrenamiento ganada al tensorizar tanto en clases como en observaciones a la hora de hacer las predicciones para testear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
